{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(265)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Database CIFAR-10 \n",
    "We load the database and only keep the airplane and bird subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Cargar el conjunto de datos CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalización\n",
    "])\n",
    "\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "img, label = full_train_dataset[0]  # Obtener la primera imagen y etiqueta\n",
    "\n",
    "print(\"Print the first image\")\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # desnormalizar\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "imshow(img)\n",
    "\n",
    "\n",
    "# Paso 2: Analizar el conjunto de datos (opcional)\n",
    "classes = full_train_dataset.classes\n",
    "print(\"Classes in CIFAR-10:\", classes)\n",
    "\n",
    "# Paso 3: Preprocesamiento ya incluido en el transform al llamar a datasets\n",
    "\n",
    "# Paso 4: Dividir el conjunto de entrenamiento en entrenamiento y validación\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "validation_size = len(full_train_dataset) - train_size\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(full_train_dataset, [train_size, validation_size])\n",
    "\n",
    "# Paso 5: Filtrar por etiquetas \"bird\" y \"plane\"\n",
    "def filter_by_label(dataset, classes_to_keep=['airplane', 'bird']):\n",
    "    targets = dataset.dataset.targets if isinstance(dataset, Subset) else dataset.targets\n",
    "    mask = [target in classes_to_keep for target in targets]\n",
    "    indices = [i for i, m in enumerate(mask) if m]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# Filtrando los conjuntos de datos\n",
    "train_dataset = filter_by_label(train_dataset, [0, 2]) # 0 es 'airplane', 2 es 'bird' en CIFAR-10\n",
    "validation_dataset = filter_by_label(validation_dataset, [0, 2])\n",
    "test_dataset = filter_by_label(test_dataset, [0, 2])\n",
    "\n",
    "# Cargadores de datos para iterar sobre los conjuntos\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(validation_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyMLP class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMLP, self).__init__()\n",
    "        # Definir las capas del MLP\n",
    "        self.fc1 = nn.Linear(3072, 512)  # Capa de entrada a la primera capa oculta\n",
    "        self.fc2 = nn.Linear(512, 128)   # Primera capa oculta a la segunda capa oculta\n",
    "        self.fc3 = nn.Linear(128, 32)    # Segunda capa oculta a la tercera capa oculta\n",
    "        self.fc4 = nn.Linear(32, 2)      # Tercera capa oculta a la capa de salida\n",
    "        # Función de activación ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Aplanar la entrada para que tenga dimensiones (batch_size, 3072)\n",
    "        x = x.view(-1, 3072)\n",
    "        # Pasar por las capas con activación ReLU para las capas ocultas\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        # La capa de salida no lleva activación porque se usará con CrossEntropyLoss\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    model.train()  # Poner el modelo en modo de entrenamiento\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for img, labels in train_loader:\n",
    "            # Change imgs.to(device=device) to imgs.to(device=device, dtype=torch.double) in your training functions and when computing accuracies in order to convert your images to the right datatype.\n",
    "            img = img.to(dtype=torch.double)\n",
    "            optimizer.zero_grad()  # Limpia los gradientes de todos los parámetros optimizados\n",
    "            outputs = model(img)  # Calcula la predicción del modelo\n",
    "            loss = loss_fn(outputs, labels)  # Calcula la pérdida\n",
    "            loss.backward()  # Realiza la retropropagación para calcular los gradientes\n",
    "            optimizer.step()  # Actualiza los parámetros del modelo\n",
    "\n",
    "            total_loss += loss.item()  # Acumula la pérdida (opcional) \n",
    "\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {total_loss/len(train_loader)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nglm-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ae36e8c2cbd9e14d80419493f2540eab6c211be174ac39ce04705a74740d0d8b"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
