{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                          #converts to tensor\n",
    "    transforms.Lambda(lambda x: torch.flatten(x))   #Flattens the tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_keep = ['bird', 'airplane']\n",
    "class_indices = [trainset.class_to_idx[c] for c in classes_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_trainset = Subset(trainset, [i for i in range(len(trainset)) if trainset.targets[i] in class_indices])\n",
    "subset_testset = Subset(testset, [i for i in range(len(testset)) if testset.targets[i] in class_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(subset_trainset))\n",
    "val_size = len(subset_trainset) - train_size\n",
    "trainset, valset = random_split(subset_trainset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=32)\n",
    "valloader = DataLoader(valset, batch_size=32)\n",
    "testloader = DataLoader(subset_testset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(3072, 512)  # Fully connected layer 1\n",
    "        self.fc2 = nn.Linear(512, 128)   # Fully connected layer 2\n",
    "        self.fc3 = nn.Linear(128, 32)    # Fully connected layer 3\n",
    "        self.fc4 = nn.Linear(32, 2)      # Fully connected  output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagation via ReLU\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # No activation function at the output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Reset gradients in each iteration\n",
    "\n",
    "            outputs = model(inputs)  # Forward propagation\n",
    "            labels = torch.Tensor([[1.0, 0.0] if i.item() == 0 else [0.0, 1.0] for i in labels])\n",
    "            loss = loss_fn(outputs, labels)  # Loss calculation\n",
    "            \n",
    "            loss.backward()  # Back propagation\n",
    "            optimizer.step()  # Update parameters\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch}/{n_epochs}], Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, model, loss_fn, train_loader, lr, weight_decay=0.0, momentum=0.0):\n",
    "    velocities ={i: 0 for i, p in enumerate(model.parameters())}\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            model.train()  # Make sure the model is in train \n",
    "            outputs = model(inputs)  # Forward Propagation\n",
    "\n",
    "            labels = torch.Tensor([[1.0, 0.0] if i.item() == 0 else [0.0, 1.0] for i in labels])\n",
    "            loss = loss_fn(outputs, labels)  # Loss Calculation\n",
    "\n",
    "            # Calculate gradients manualy\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters manually using learning rate\n",
    "            with torch.no_grad():\n",
    "                for i, param in enumerate(model.parameters()):\n",
    "                    gradient = param.grad\n",
    "\n",
    "                    if weight_decay != 0:\n",
    "                        gradient = gradient.add_(param.data, alpha=weight_decay)\n",
    "\n",
    "                    if momentum != 0:\n",
    "                        velocities[i] = velocities[i] * momentum + gradient\n",
    "                        gradient = velocities[i]\n",
    "                    \n",
    "                    new_param = param.data.add_(gradient, alpha=-lr)\n",
    "                    param.copy_(new_param)\n",
    "                    param.grad.zero_()\n",
    "\n",
    "            # Reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Average loss per epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch}/{n_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # We return the final loss value in able to afterwards know the best aproach\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6920\n",
      "Epoch [2/10], Loss: 0.6877\n",
      "Epoch [3/10], Loss: 0.6848\n",
      "Epoch [4/10], Loss: 0.6819\n",
      "Epoch [5/10], Loss: 0.6789\n",
      "Epoch [6/10], Loss: 0.6755\n",
      "Epoch [7/10], Loss: 0.6717\n",
      "Epoch [8/10], Loss: 0.6673\n",
      "Epoch [9/10], Loss: 0.6623\n",
      "Epoch [10/10], Loss: 0.6563\n",
      "Epoch [1/10], Loss: 0.6793\n",
      "Epoch [2/10], Loss: 0.6587\n",
      "Epoch [3/10], Loss: 0.6276\n",
      "Epoch [4/10], Loss: 0.5856\n",
      "Epoch [5/10], Loss: 0.5553\n",
      "Epoch [6/10], Loss: 0.5409\n",
      "Epoch [7/10], Loss: 0.5340\n",
      "Epoch [8/10], Loss: 0.5302\n",
      "Epoch [9/10], Loss: 0.5280\n",
      "Epoch [10/10], Loss: 0.5265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5264940635390785"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "model = MyMLP()\n",
    "model_manual = MyMLP()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "train(n_epochs, optimizer, model, criterion, train_loader)\n",
    "train_manual_update(n_epochs, model_manual, criterion, train_loader, lr, weight_decay=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (train): 0.6526539619248303\n",
      "Loss (train_manual_update): 0.5233056174851183\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model_manual.eval()\n",
    "\n",
    "epoch_loss_train = 0.0\n",
    "epoch_loss_manual = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        labels = torch.Tensor([[1.0, 0.0] if i.item() == 0 else [0.0, 1.0] for i in labels])\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss_train += loss.item() * inputs.size(0)\n",
    "\n",
    "        outputs_manual = model_manual(inputs)\n",
    "        loss_manual = criterion(outputs_manual, labels)\n",
    "        epoch_loss_manual += loss_manual.item() * inputs.size(0)\n",
    "\n",
    "epoch_loss_train /= len(train_loader.dataset)\n",
    "epoch_loss_manual /= len(train_loader.dataset)\n",
    "\n",
    "# Print loss for each model\n",
    "print(\"Loss (train):\", epoch_loss_train)\n",
    "print(\"Loss (train_manual_update):\", epoch_loss_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6909\n",
      "Epoch [2/10], Loss: 0.6864\n",
      "Epoch [3/10], Loss: 0.6825\n",
      "Epoch [4/10], Loss: 0.6782\n",
      "Epoch [5/10], Loss: 0.6731\n",
      "Epoch [6/10], Loss: 0.6668\n",
      "Epoch [7/10], Loss: 0.6579\n",
      "Epoch [8/10], Loss: 0.6485\n",
      "Epoch [9/10], Loss: 0.6379\n",
      "Epoch [10/10], Loss: 0.6256\n",
      "Epoch [1/10], Loss: 0.6602\n",
      "Epoch [2/10], Loss: 0.5659\n",
      "Epoch [3/10], Loss: 0.5195\n",
      "Epoch [4/10], Loss: 0.5060\n",
      "Epoch [5/10], Loss: 0.4971\n",
      "Epoch [6/10], Loss: 0.4897\n",
      "Epoch [7/10], Loss: 0.4824\n",
      "Epoch [8/10], Loss: 0.4748\n",
      "Epoch [9/10], Loss: 0.4665\n",
      "Epoch [10/10], Loss: 0.4577\n",
      "Epoch [1/10], Loss: 0.6864\n",
      "Epoch [2/10], Loss: 0.6786\n",
      "Epoch [3/10], Loss: 0.6707\n",
      "Epoch [4/10], Loss: 0.6577\n",
      "Epoch [5/10], Loss: 0.6332\n",
      "Epoch [6/10], Loss: 0.5996\n",
      "Epoch [7/10], Loss: 0.5743\n",
      "Epoch [8/10], Loss: 0.5602\n",
      "Epoch [9/10], Loss: 0.5524\n",
      "Epoch [10/10], Loss: 0.5479\n",
      "Epoch [1/10], Loss: 0.5604\n",
      "Epoch [2/10], Loss: 0.5134\n",
      "Epoch [3/10], Loss: 0.4920\n",
      "Epoch [4/10], Loss: 0.4744\n",
      "Epoch [5/10], Loss: 0.4626\n",
      "Epoch [6/10], Loss: 0.4510\n",
      "Epoch [7/10], Loss: 0.4430\n",
      "Epoch [8/10], Loss: 0.4337\n",
      "Epoch [9/10], Loss: 0.4281\n",
      "Epoch [10/10], Loss: 0.4184\n"
     ]
    }
   ],
   "source": [
    "# Train 4 models with different learning rates, momentum and weight decay\n",
    "models = []\n",
    "for _ in range(4):\n",
    "    model = MyMLP()\n",
    "    models.append(model)\n",
    "\n",
    "n_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "performance = []\n",
    "performance.append(train_manual_update(n_epochs, models[0], criterion, train_loader, lr=0.001, weight_decay=0.0, momentum=0.0))\n",
    "performance.append(train_manual_update(n_epochs, models[1], criterion, train_loader, lr=0.01, weight_decay=0.0, momentum=0.0))\n",
    "performance.append(train_manual_update(n_epochs, models[2], criterion, train_loader, lr=0.001, weight_decay=0.1, momentum=0.9))\n",
    "performance.append(train_manual_update(n_epochs, models[3], criterion, train_loader, lr=0.01, weight_decay=0.0, momentum=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model 4 had the best approach\n",
      "Loss validation (test): 0.4307105165541154\n"
     ]
    }
   ],
   "source": [
    "# Get the best perfomance\n",
    "bestModelIndex = performance.index(min(performance))\n",
    "print(\"The model\", bestModelIndex+1, \"had the best approach\")\n",
    "bestModel = models[bestModelIndex]\n",
    "\n",
    "# Model 4 has the best performance\n",
    "# Model 4 on unseen data\n",
    "\n",
    "bestModel.eval()\n",
    "\n",
    "epoch_loss_test = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        outputs = bestModel(inputs)\n",
    "        \n",
    "        labels = torch.Tensor([[1.0, 0.0] if i.item() == 0 else [0.0, 1.0] for i in labels])\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss_test += loss.item() * inputs.size(0)\n",
    "\n",
    "epoch_loss_test /= len(testloader.dataset)\n",
    "\n",
    "print(\"Loss validation (test):\", epoch_loss_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
