{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from utils import train, set_device, compute_accuracy\n",
    "\n",
    "torch.manual_seed(265)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Device {device}.\")\n",
    "\n",
    "# tokenizer will split a long text into a list of english words\n",
    "TOKENIZER_EN = get_tokenizer('basic_english')\n",
    "# Where we will store / load all our models, datasets, vocabulary, etc.\n",
    "PATH_GENERATED = './generated/'\n",
    "# Minimum number of occurence of a word in the text to add it to the vocabulary\n",
    "MIN_FREQ = 100\n",
    "\n",
    "# If you want to remove some characters to get rid of noise\n",
    "# specials=[\"<unk>\", \",\", \".\", \"!\", \"?\"] \n",
    "specials = []\n",
    "\n",
    "# List of posible words to use\n",
    "target_words = [\"be\", \"am\", \"are\", \"is\", \"was\", \"were\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\"]\n",
    "taget_words_size = len(target_words) \n",
    "\n",
    "\n",
    "def read_files(datapath='./data_train/'):\n",
    "    \"\"\"\n",
    "    Return a list of strings, one for each line in each .txt files in 'datapath'\n",
    "    \"\"\"\n",
    "    # Find all txt files in directory \n",
    "    files = os.listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith(\".txt\")]\n",
    "    \n",
    "    # Stores each line of each book in a list\n",
    "    lines = []\n",
    "    for f_name in files:\n",
    "        with open(f_name) as f:\n",
    "            lines += f.readlines()\n",
    "    return lines\n",
    "\n",
    "def tokenize(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Tokenize the list of lines\n",
    "    \"\"\"\n",
    "    list_text = []\n",
    "    for line in lines:\n",
    "        list_text += tokenizer(line)\n",
    "    return list_text\n",
    "\n",
    "def yield_tokens(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Yield tokens, ignoring names and digits to build vocabulary\n",
    "    \"\"\"\n",
    "    # Match any word containing digit\n",
    "    no_digits = '\\w*[09]+\\w*'\n",
    "    # Match word containing a uppercase \n",
    "    no_names = '\\w*[A-Z]+\\w*'\n",
    "    # Match any sequence containing more than one space\n",
    "    no_spaces = '\\s+'\n",
    "    \n",
    "    for line in lines:\n",
    "        # Remove special characters\n",
    "        line = ''.join([char for char in line if char not in specials])\n",
    "        line = re.sub(no_digits, ' ', line)\n",
    "        line = re.sub(no_names, ' ', line)\n",
    "        line = re.sub(no_spaces, ' ', line)\n",
    "        yield tokenizer(line)\n",
    "\n",
    "def count_freqs(words, vocab):\n",
    "    \"\"\"\n",
    "    Count occurrences of each word in vocabulary in the data\n",
    "    \n",
    "    Useful to get some insight on the data and to compute loss weights\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_stoi = vocab.get_stoi()\n",
    "\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        if w in vocab_stoi:\n",
    "            freqs[vocab[w]] += 1\n",
    "    return freqs\n",
    "\n",
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    \"\"\"\n",
    "    Create a vocabulary (list of known tokens) from a list of strings\n",
    "    \"\"\"\n",
    "    # vocab contains the vocabulary found in the data, associating an index to each word\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"])\n",
    "    # Since we removed all words with an uppercase when building the vocabulary, we skipped the word \"I\"\n",
    "    vocab.append_token(\"i\")\n",
    "    # Value of default index. This index will be returned when OOV (Out Of Vocabulary) token is queried.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    # SANITAZE THE INPUT, Get rid of the unkown words \n",
    "\n",
    "    return vocab\n",
    "\n",
    "# ----------------------- Tokenize texts -------------------------------\n",
    "# Load tokenized versions of texts if you have already generated it\n",
    "# Otherwise, create it and save it\n",
    "if os.path.isfile(PATH_GENERATED + \"words_train.pt\"):\n",
    "    words_train = torch.load(PATH_GENERATED + \"words_train.pt\")\n",
    "    words_val = torch.load(PATH_GENERATED + \"words_val.pt\")\n",
    "    words_test = torch.load(PATH_GENERATED + \"words_test.pt\")\n",
    "else:\n",
    "    # Get lists of strings, one for each line in each .txt files in 'datapath' \n",
    "    lines_books_train = read_files('./data_train/')\n",
    "    lines_books_val = read_files('./data_val/')\n",
    "    lines_books_test = read_files('./data_test/')\n",
    "\n",
    "    # List of words contained in the dataset\n",
    "    words_train = tokenize(lines_books_train)\n",
    "    words_val = tokenize(lines_books_val)\n",
    "    words_test = tokenize(lines_books_test)\n",
    "    \n",
    "    torch.save(words_train , PATH_GENERATED + \"words_train.pt\")\n",
    "    torch.save(words_val , PATH_GENERATED + \"words_val.pt\")\n",
    "    torch.save(words_test , PATH_GENERATED + \"words_test.pt\")\n",
    "\n",
    "\n",
    "# ----------------------- Create vocabulary ----------------------------\n",
    "VOCAB_FNAME = \"vocabulary.pt\"\n",
    "# Load vocabulary if you have already generated it\n",
    "# Otherwise, create it and save it\n",
    "if os.path.isfile(PATH_GENERATED + VOCAB_FNAME):\n",
    "    vocab = torch.load(PATH_GENERATED + VOCAB_FNAME)\n",
    "else:\n",
    "    # Create vocabulary based on the words in the training dataset\n",
    "    vocab = create_vocabulary(lines_books_train, min_freq=MIN_FREQ)\n",
    "    torch.save(vocab, PATH_GENERATED + VOCAB_FNAME)\n",
    "    \n",
    "\n",
    "\n",
    "vocab_itos = vocab.get_itos()\n",
    "# To get the list of words as a list\n",
    "\n",
    "def filter_text(words, vocab):\n",
    "    \"\"\"\n",
    "    Filters a list of words, keeping only those that are in the given vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        words (list of str): The list of words to filter.\n",
    "        vocab (Vocab): The vocabulary object with .stoi attribute for checking word indices.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list containing only the words that are in the vocabulary.\n",
    "    \"\"\"\n",
    "    # Keep only words that are in the vocabulary\n",
    "    return [word for word in words if word in vocab_itos]\n",
    "\n",
    "\n",
    "# ----------------------- Tokenize texts -------------------------------\n",
    "# Load tokenized versions of texts if you have already generated it\n",
    "# Otherwise, create it and save it\n",
    "FILTERED_WORDS_TRAIN_PATH = \"filtered_words_train.pt\"\n",
    "FILTERED_WORDS_VAL_PATH = \"filtered_words_val.pt\"\n",
    "FILTERED_WORDS_TEST_PATH = \"filtered_words_test.pt\"\n",
    "if os.path.isfile(PATH_GENERATED + FILTERED_WORDS_TRAIN_PATH):\n",
    "    filtered_words_train = torch.load(PATH_GENERATED + FILTERED_WORDS_TRAIN_PATH)\n",
    "    filtered_words_val = torch.load(PATH_GENERATED + FILTERED_WORDS_VAL_PATH)\n",
    "    filtered_words_test = torch.load(PATH_GENERATED + FILTERED_WORDS_TEST_PATH)\n",
    "else:\n",
    "    # Example usage with training, validation, and test datasets\n",
    "    filtered_words_train = filter_text(words_train, vocab)\n",
    "    filtered_words_val = filter_text(words_val, vocab)\n",
    "    filtered_words_test = filter_text(words_test, vocab)\n",
    "\n",
    "        \n",
    "    torch.save(filtered_words_train , PATH_GENERATED + FILTERED_WORDS_TRAIN_PATH )\n",
    "    torch.save(filtered_words_val , PATH_GENERATED + FILTERED_WORDS_VAL_PATH)\n",
    "    torch.save(filtered_words_test , PATH_GENERATED + FILTERED_WORDS_TEST_PATH)\n",
    "\n",
    "\n",
    "# ------------------------ Quick analysis  ------------------------------\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "TARGET_OUTPUT = [\"be\", \"am\", \"are\", \"is\", \"was\", \"were\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\"]\n",
    "\n",
    "print(\"------------Normal Database VS Filtered Database---------------\")\n",
    "print(\"Total number of words in the training dataset:             \", len(words_train))\n",
    "print(\"Total number of words in the filtered training dataset:    \", len(filtered_words_train))\n",
    "print(\"Total number of words in the validation dataset:           \", len(words_val))\n",
    "print(\"Total number of words in the filtered validation dataset:  \", len(filtered_words_val))\n",
    "print(\"Total number of words in the test dataset:                 \", len(words_test))\n",
    "print(\"Total number of words in filtered the test dataset:        \", len(filtered_words_test))\n",
    "print(\"Number of distinct words in the training dataset:          \", len(set(words_train)))\n",
    "print(\"Number of distinct words in the filtered training dataset: \", len(set(filtered_words_train)))\n",
    "print(\"Number of distinct words kept (vocabulary size):           \", VOCAB_SIZE)\n",
    "\n",
    "print(\"Possible words prediction: \", len(TARGET_OUTPUT),\", \", TARGET_OUTPUT)\n",
    "\n",
    "freqs = count_freqs(words_train, vocab)\n",
    "\n",
    "occurrences = [(f.item(), w) for (f, w) in zip(freqs, vocab.lookup_tokens(range(VOCAB_SIZE)))]\n",
    "specials = [\"<unk>\", \",\", \".\", \"!\", \"?\"]\n",
    "\n",
    "# Filtrar las ocurrencias para eliminar palabras especiales\n",
    "filtered_occurrences = [(freq, word) for freq, word in occurrences if word not in specials]\n",
    "top_20_occurrences = sorted(filtered_occurrences, key=lambda x: x[0], reverse=True)[:20]\n",
    "\n",
    "\n",
    "print(\"20 highest appareances are: \")\n",
    "for freq, word in top_20_occurrences:\n",
    "    print(\" {\"+f\"{word}: {freq}\"+\"}\")\n",
    "\n",
    "print(type(vocab))\n",
    "    \n",
    "    \n",
    "\n",
    "def create_dataset(text, vocab, context_size):\n",
    "    \"\"\"\n",
    "    Create a dataset from a list of context words and a list of target words\n",
    "    \"\"\"\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    n_text = len(text)\n",
    "\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    for i in range(n_text - context_size):\n",
    "\n",
    "        t = txt[i + context_size]\n",
    "\n",
    "        c = torch.Tensor(txt[i : i + context_size]).type(torch.long)\n",
    "\n",
    "        targets.append(t)\n",
    "        contexts.append(c)\n",
    "\n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets)\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "def load_dataset(words, vocab, fname, context_size):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    # If already generated\n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname)\n",
    "    else:\n",
    "        # Create context / target dataset based on the list of strings\n",
    "        dataset = create_dataset(words, vocab, context_size)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "context_size = 5\n",
    "batch_size = 64\n",
    "\n",
    "# Normal Data Train\n",
    "data_train = load_dataset(words_train, vocab, \"data_train.pt\", context_size)\n",
    "data_val = load_dataset(words_val, vocab, \"data_val.pt\", context_size)\n",
    "data_test = load_dataset(words_test, vocab, \"data_test.pt\", context_size)\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(data_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Filtered Data Train\n",
    "filtered_data_train = load_dataset(filtered_words_train, vocab, \"filtered_data_train.pt\", context_size)\n",
    "filtered_data_val = load_dataset(filtered_words_val, vocab, \"filtered_data_val.pt\", context_size)\n",
    "filtered_data_test = load_dataset(filtered_words_test, vocab, \"filtered_data_test.pt\", context_size)\n",
    "\n",
    "filtered_train_loader = DataLoader(filtered_data_train, batch_size=batch_size, shuffle=False)\n",
    "filtered_val_loader = DataLoader(filtered_data_val, batch_size=batch_size, shuffle=False)\n",
    "filtered_test_loader = DataLoader(filtered_data_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Acceder a la palabra correspondiente al índice 0\n",
    "word_at_index_0 = vocab_itos[0]\n",
    "print(f\"The word with index 0: {word_at_index_0}\")\n",
    "\n",
    "index = (20,23)\n",
    "\n",
    "print(\" NORMAL DATASBASE\")\n",
    "for i in range(*index):\n",
    "    input_tensor, target_tensor = data_train[i]\n",
    "    input_tensor = input_tensor.tolist()\n",
    "    target_tensor = target_tensor.item()\n",
    "\n",
    "    # Para el tensor objetivo\n",
    "    phrase = \"\"\n",
    "\n",
    "    for i,value in enumerate(input_tensor):\n",
    "        phrase += vocab_itos[value]+ \" \"\n",
    "    \n",
    "    phrase += vocab_itos[target_tensor]\n",
    "    print(\"Input: \", input_tensor, \"-> \", phrase, \" Target: \", target_tensor)\n",
    "\n",
    "print(\" FILTERED DATASBASE\")\n",
    "for i in range(*index):\n",
    "    input_tensor, target_tensor = filtered_data_train[i]\n",
    "    input_tensor = input_tensor.tolist()\n",
    "    target_tensor = target_tensor.item()\n",
    "\n",
    "    # Para el tensor objetivo\n",
    "    phrase = \"\"\n",
    "\n",
    "    for i,value in enumerate(input_tensor):\n",
    "        phrase += vocab_itos[value]+ \" \"\n",
    "    \n",
    "    phrase += vocab_itos[target_tensor]\n",
    "    print(\"Input: \", input_tensor, \"-> \", phrase, \" Target: \", target_tensor)\n",
    "        \n",
    "\n",
    "class NGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((inputs.shape[0], -1))\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \"\"\"\n",
    "    Train our model and save weight values\n",
    "    \"\"\"\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        loss_train = 0.0\n",
    "        for contexts, labels in train_loader:\n",
    "\n",
    "            contexts = contexts.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(contexts)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        print(\n",
    "            \"{}  |  Epoch {}  |  Training loss {:.5f}\".format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch\n",
    "            )\n",
    "        )\n",
    "    return\n",
    "\n",
    "\n",
    "def relative_error(a, b):\n",
    "    return torch.norm(a - b) / torch.norm(a)\n",
    "\n",
    "titles = []\n",
    "models = []\n",
    "\n",
    "\n",
    "NGRAM_PATH = \"NGram.pt\"\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + NGRAM_PATH):\n",
    "    NGram_model = torch.load(PATH_GENERATED + NGRAM_PATH)\n",
    "else:\n",
    "    loss_fn = nn.NLLLoss()\n",
    "\n",
    "    n_epochs = 4\n",
    "    lr = 0.1\n",
    "    embedding_dim = 16\n",
    "\n",
    "    NGram_model = NGram(len(vocab), embedding_dim, context_size).to(device=device)\n",
    "    optimizer = optim.SGD(NGram_model.parameters(), lr=lr)\n",
    "\n",
    "    weight = train(\n",
    "        n_epochs=n_epochs,\n",
    "        optimizer=optimizer,\n",
    "        model=NGram_model,\n",
    "        loss_fn=loss_fn,\n",
    "        train_loader=train_loader,\n",
    "    )\n",
    "    \n",
    "    torch.save(NGram_model , PATH_GENERATED + NGRAM_PATH)\n",
    "\n",
    "titles.append(\"NGram\")\n",
    "models.append(NGram_model)\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of model with given data loader\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for contexts, labels in loader:\n",
    "            contexts = contexts.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def model_selection(models, titles, train_loader, val_loader):\n",
    "    \"\"\"\n",
    "    Choose the best model from the list of models based on validation accuracy\n",
    "    \"\"\"\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "    for model, title in zip(models, titles):\n",
    "        acc = accuracy(model, val_loader)\n",
    "        train_acc = accuracy(model, train_loader)\n",
    "        print(f\"{title} | Train accuracy {train_acc:.2%} |  Validation accuracy {acc:.2%}\")\n",
    "        if acc > best_acc:\n",
    "            best_model = model\n",
    "            best_acc = acc\n",
    "    return best_model\n",
    "\n",
    "best_model = model_selection(\n",
    "    models, titles, train_loader, val_loader\n",
    ")\n",
    "\n",
    "print(best_model)\n",
    "\n",
    "best_model_acc = accuracy(best_model, test_loader)\n",
    "print(f\"Best conjugating model | Test accuracy {best_model_acc:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "SIM_MATRIX_PATH = \"SimilarityMatrix.pt\"\n",
    "best_model_weights = best_model.embeddings.weight.detach()\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + SIM_MATRIX_PATH):\n",
    "    sim_matrix = torch.load(PATH_GENERATED + SIM_MATRIX_PATH)\n",
    "else:\n",
    "    sim_matrix = np.zeros(shape=(len(best_model_weights), len(best_model_weights)))\n",
    "\n",
    "    for w1 in range(0, len(best_model_weights)):\n",
    "        for w2 in range(0, len(best_model_weights)):\n",
    "            # Cosine similarity\n",
    "            sim_matrix[w1][w2] = np.dot(best_model_weights[w1], best_model_weights[w2]) / (\n",
    "                np.linalg.norm(best_model_weights[w1]) * np.linalg.norm(best_model_weights[w2])\n",
    "            )\n",
    "    \n",
    "    torch.save(sim_matrix , PATH_GENERATED + SIM_MATRIX_PATH)\n",
    "\n",
    "\n",
    "testing_words = [\"me\", \"white\", \"man\", \"have\", \"be\", \"child\", \"yes\", \"what\"]\n",
    "testing_words_idx = [(i, vocab.get_stoi()[i]) for i in testing_words]\n",
    "\n",
    "n_most_similar = 10\n",
    "for w in testing_words_idx:\n",
    "\n",
    "    indices = (-sim_matrix[w[1]]).argsort()[:n_most_similar]\n",
    "    sim_words = [(vocab.get_itos()[i], sim_matrix[w[1]][i]) for i in indices]\n",
    "    print(f\"{w} most similar words:\")\n",
    "\n",
    "    for sim_w in sim_words:\n",
    "        print(f\"\\t{sim_w}\")\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(PATH_GENERATED +\"./weights.tsv\", \"wt\") as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter=\"\\t\")\n",
    "    for i in best_model_weights:\n",
    "        tsv_writer.writerow(i.numpy())\n",
    "\n",
    "\n",
    "with open(PATH_GENERATED +\"./metadata.tsv\", \"wt\") as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter=\"\\t\")\n",
    "    vocab_dict = vocab.get_itos()\n",
    "    for i in vocab_dict:\n",
    "        tsv_writer.writerow([i])\n",
    "        \n",
    "from torchtext.vocab import vocab as Vocab\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "counter = Counter(words_train)\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "target_output = []\n",
    "\n",
    "for element in sorted_by_freq_tuples:\n",
    "    # Put the chosen ones at the beggining\n",
    "    if element[0] in TARGET_OUTPUT:\n",
    "        target_output.insert(0, element)\n",
    "    else:\n",
    "        target_output.append(element)\n",
    "\n",
    "ordered_dict = OrderedDict(target_output)\n",
    "specials = TARGET_OUTPUT + ['<unk>']\n",
    "vocab_conjugate = Vocab(ordered_dict, specials=specials)\n",
    "vocab_conjugate.set_default_index(vocab_conjugate['<unk>'])\n",
    "vocab_conjugate_dict = vocab_conjugate.get_itos()\n",
    "\n",
    "target_to_idx = {target: i for i, target in enumerate(TARGET_OUTPUT)}\n",
    "\n",
    "# Crear un diccionario para guardar las palabras y sus índices\n",
    "target_to_idx = {}\n",
    "\n",
    "# Buscar cada palabra y obtener su índice\n",
    "for palabra in TARGET_OUTPUT:\n",
    "    try:\n",
    "        indice = vocab_conjugate_dict.index(palabra)\n",
    "        target_to_idx[palabra] = indice\n",
    "    except ValueError:\n",
    "        print(f\"La palabra '{palabra}' no se encontró en la lista.\")\n",
    "        target_to_idx[palabra] = None\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Índices de las palabras buscadas:\")\n",
    "for i, (palabra, indice) in enumerate(target_to_idx.items()):\n",
    "    if indice is not None:\n",
    "        print(f\"{i}: '{palabra}' está en el índice {indice}.\")\n",
    "    else:\n",
    "        print(f\"{i}: '{palabra}' no está en la lista.\")\n",
    "\n",
    "\n",
    "def create_aroundTarget_dataset(text, vocab, before_context_size, after_context_size):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    lenText = len(text)\n",
    "\n",
    "    for i in range(before_context_size, lenText - after_context_size):\n",
    "\n",
    "        word = text[i]\n",
    "\n",
    "        if word in TARGET_OUTPUT:\n",
    "\n",
    "            t = target_to_idx[word]\n",
    "            \n",
    "\n",
    "            around = (\n",
    "                text[i - before_context_size - 1 : i - 1]\n",
    "                + text[i + 1 : i + after_context_size + 1]\n",
    "            )\n",
    "\n",
    "            c = torch.Tensor([vocab[w] for w in around]).type(torch.long)\n",
    "\n",
    "            # The index of the target has to be from 0 to 11 not the vocab one\n",
    "            targets.append(t)\n",
    "\n",
    "            contexts.append(c)\n",
    "\n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets)\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "before_context_size=16\n",
    "after_context_size=4\n",
    "batch_size = 1024\n",
    "\n",
    "train_conjugate_data = create_aroundTarget_dataset(words_train, vocab_conjugate, before_context_size, after_context_size)\n",
    "val_conjugate_data = create_aroundTarget_dataset(words_val, vocab_conjugate, before_context_size, after_context_size)\n",
    "test_conjugate_data = create_aroundTarget_dataset(words_test, vocab_conjugate, before_context_size, after_context_size)\n",
    "\n",
    "filtered_train_conjugate_data = create_aroundTarget_dataset(filtered_words_train, vocab_conjugate, before_context_size, after_context_size)\n",
    "filtered_val_conjugate_data = create_aroundTarget_dataset(filtered_words_val, vocab_conjugate, before_context_size, after_context_size)\n",
    "filtered_test_conjugate_data = create_aroundTarget_dataset(filtered_words_test, vocab_conjugate, before_context_size, after_context_size)\n",
    "\n",
    "train_conjugate_data_loader = DataLoader(train_conjugate_data, batch_size=batch_size)\n",
    "val_conjugate_data_loader = DataLoader(val_conjugate_data, batch_size=batch_size)\n",
    "test_conjugate_data_loader = DataLoader(test_conjugate_data, batch_size=batch_size)\n",
    "\n",
    "filtered_train_conjugate_data_loader = DataLoader(filtered_train_conjugate_data, batch_size=batch_size)\n",
    "filtered_val_conjugate_data_loader = DataLoader(filtered_val_conjugate_data, batch_size=batch_size)\n",
    "filtered_test_conjugate_data_loader = DataLoader(filtered_test_conjugate_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"TARGET: \")\n",
    "print(target_to_idx)\n",
    "\n",
    "# Acceder a la palabra correspondiente al índice 0\n",
    "word_at_index_0 = vocab_conjugate_dict[0]\n",
    "print(f\"The word with index 0: {word_at_index_0}\\n\")\n",
    "\n",
    "index = (200, 210)\n",
    "\n",
    "print(\" NORMAL DATABASE\")\n",
    "for i in range(*index):\n",
    "    input_tensor, target_tensor = train_conjugate_data[i]\n",
    "    input_tensor = input_tensor.tolist()\n",
    "    target_tensor = target_tensor.item()\n",
    "\n",
    "    # Inicializar la frase\n",
    "    phrase = \"\"\n",
    "\n",
    "    # Añadir las primeras 5 palabras del input_tensor a la frase\n",
    "    for value in input_tensor[:before_context_size]:\n",
    "        phrase += vocab_conjugate_dict[value] + \" \"\n",
    "\n",
    "    # Añadir la palabra del target_tensor a la frase\n",
    "    phrase += \" *\"+ vocab_conjugate_dict[target_tensor] + \"* \"\n",
    "\n",
    "    # Añadir los últimos 2 valores del input_tensor a la frase\n",
    "    for value in input_tensor[-after_context_size:]:\n",
    "        phrase += vocab_conjugate_dict[value] + \" \"\n",
    "\n",
    "    # Imprimir la frase completa\n",
    "    print(\"Input: \", input_tensor,\" Target: \", target_tensor, \"-> {\", vocab_conjugate_dict[target_tensor], \"}\"\"\\n   -> {\", phrase.strip(),  \"}\")\n",
    "\n",
    "print(\"\\n FILTERED DATABASE\")\n",
    "for i in range(*index):\n",
    "    input_tensor, target_tensor = filtered_train_conjugate_data[i]\n",
    "    input_tensor = input_tensor.tolist()\n",
    "    target_tensor = target_tensor.item()\n",
    "\n",
    "    # Inicializar la frase\n",
    "    phrase = \"\"\n",
    "\n",
    "    # Añadir las primeras 5 palabras del input_tensor a la frase\n",
    "    for value in input_tensor[:before_context_size]:\n",
    "        phrase += vocab_conjugate_dict[value] + \" \"\n",
    "\n",
    "    # Añadir la palabra del target_tensor a la frase\n",
    "    phrase += \"*\"+ vocab_conjugate_dict[target_tensor] + \"* \"\n",
    "\n",
    "    # Añadir los últimos 2 valores del input_tensor a la frase\n",
    "    for value in input_tensor[-after_context_size:]:\n",
    "        phrase += vocab_conjugate_dict[value] + \" \"\n",
    "\n",
    "    # Imprimir la frase completa\n",
    "    print(\"Input: \", input_tensor,\" Target: \", target_tensor, \"-> {\", vocab_conjugate_dict[target_tensor], \"}\"\"\\n   -> {\", phrase.strip(),  \"}\")\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        last_hidden = output[:, -1, :]\n",
    "        out = self.fc(last_hidden)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            _, predicted_index = torch.max(output, 1)\n",
    "        return predicted_index\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# rnn_model.predict(input_tensor)\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            _, predicted_index = torch.max(output, 1)\n",
    "        return predicted_index\n",
    "\n",
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, device=None):\n",
    "    device = set_device(device)\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        loss_train = 0.0\n",
    "        for contexts, targets in train_loader:\n",
    "\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train\n",
    "\n",
    "models = []\n",
    "titles = []\n",
    "batch_size = 1024\n",
    "\n",
    "MLP_MODEL_PATH = \"MLP_model.pt\"\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + MLP_MODEL_PATH):\n",
    "    mlp_model = torch.load(PATH_GENERATED + MLP_MODEL_PATH)\n",
    "else:\n",
    "    embedding_dim = 16\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_dim = 20 * embedding_dim  # Each word is embedded into embedding_dim dimensions\n",
    "    hidden_dim = 100\n",
    "    output_dim = len(TARGET_OUTPUT)  # 12 possible target words\n",
    "\n",
    "    # Instantiate the model\n",
    "    mlp_model = MLPModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    n_epochs = 4\n",
    "    lr = 0.001\n",
    "    optimizer = optim.SGD(mlp_model.parameters(), lr=lr)\n",
    "\n",
    "    train(n_epochs, optimizer, mlp_model, loss_fn, train_conjugate_data_loader)\n",
    "    torch.save(mlp_model , PATH_GENERATED + MLP_MODEL_PATH)\n",
    "\n",
    "models.append(mlp_model)\n",
    "titles.append(\"MLP\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "RNN_MODEL_PATH = \"RNN_model.pt\"\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + RNN_MODEL_PATH):\n",
    "    rnn_model = torch.load(PATH_GENERATED + RNN_MODEL_PATH)\n",
    "else:\n",
    "    # Hyperparameters\n",
    "    vocab_size = len(vocab_conjugate)  # Depends on your vocabulary\n",
    "    embedding_dim = 16\n",
    "    hidden_dim = 100\n",
    "    output_dim = len(TARGET_OUTPUT)  # 12 possible target words\n",
    "\n",
    "    # Instantiate the model\n",
    "    rnn_model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "    # Definir la función de pérdida y el optimizador\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    n_epochs = 10\n",
    "    lr = 0.001\n",
    "    optimizer = optim.Adam(rnn_model.parameters(), lr=lr)\n",
    "\n",
    "    # Función de entrenamiento (suponiendo que está definida para aceptar estos parámetros)\n",
    "    train(n_epochs, optimizer, rnn_model, loss_fn, train_conjugate_data_loader)\n",
    "\n",
    "    # Guardar el modelo entrenado\n",
    "    torch.save(rnn_model, PATH_GENERATED + RNN_MODEL_PATH)\n",
    "\n",
    "# Si 'models' y 'titles' son listas para guardar los modelos y sus nombres\n",
    "models.append(rnn_model)\n",
    "titles.append(\"RNN\")\n",
    "\n",
    "\n",
    "best_conjugating_model = model_selection(\n",
    "    models, titles, train_conjugate_data_loader, val_conjugate_data_loader\n",
    ")\n",
    "\n",
    "print(best_conjugating_model)\n",
    "\n",
    "best_conjugating_model_acc = accuracy(best_conjugating_model, test_loader)\n",
    "print(f\"Best conjugating model | Test accuracy {best_conjugating_model_acc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphapp-CGLyoVIh-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "cd17699b0f8c8ba78222bc090e4903241cbfccfe2c3d9b347e7b5dfb6ba74475"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
